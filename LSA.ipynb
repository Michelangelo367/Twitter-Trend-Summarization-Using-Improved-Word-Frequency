{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'document':documents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although I realize that principle is not one of your strongest\\npoints, I would still like to know why do do not ask any question\\nof this sort about the Arab countries.\\n\\n   If you want to conti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Notwithstanding all the legitimate fuss about this proposal, how much\\nof a change is it?  ATT's last product in this area (a) was priced over\\n$1000, as I suspect 'clipper' phones will be; (b) ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, I will have to change the scoring on my playoff pool.  Unfortunately\\nI don't have time right now, but I will certainly post the new scoring\\nrules by tomorrow.  Does it matter?  No, you'll ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                  document\n",
       "0  Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe...\n",
       "1  \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n...\n",
       "2  Although I realize that principle is not one of your strongest\\npoints, I would still like to know why do do not ask any question\\nof this sort about the Arab countries.\\n\\n   If you want to conti...\n",
       "3  Notwithstanding all the legitimate fuss about this proposal, how much\\nof a change is it?  ATT's last product in this area (a) was priced over\\n$1000, as I suspect 'clipper' phones will be; (b) ca...\n",
       "4  Well, I will have to change the scoring on my playoff pool.  Unfortunately\\nI don't have time right now, but I will certainly post the new scoring\\nrules by tomorrow.  Does it matter?  No, you'll ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing everything except alphabets`\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# removing short words\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# make all text lowercase\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# tokenization\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
    "\n",
    "# remove stop-words\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "# de-tokenization\n",
    "detokenized_doc = []\n",
    "for i in range(len(news_df)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "news_df['clean_doc'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>clean_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe...</td>\n",
       "      <td>well sure story seem biased disagree statement media ruin israels reputation rediculous media israeli media world lived europe realize incidences described letter occured media whole seem ignore s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n...</td>\n",
       "      <td>yeah expect people read actually accept hard atheism need little leap faith jimmy logic runs steam sorry pity sorry feelings denial faith need well pretend happily ever anyway maybe start newsgrou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although I realize that principle is not one of your strongest\\npoints, I would still like to know why do do not ask any question\\nof this sort about the Arab countries.\\n\\n   If you want to conti...</td>\n",
       "      <td>although realize principle strongest points would still like know question sort arab countries want continue think tank charade fixation israel must stop might start asking sort questions arab cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Notwithstanding all the legitimate fuss about this proposal, how much\\nof a change is it?  ATT's last product in this area (a) was priced over\\n$1000, as I suspect 'clipper' phones will be; (b) ca...</td>\n",
       "      <td>notwithstanding legitimate fuss proposal much change last product area priced suspect clipper phones came customer automatically preregistered government authorities thus aside attempting legitimi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, I will have to change the scoring on my playoff pool.  Unfortunately\\nI don't have time right now, but I will certainly post the new scoring\\nrules by tomorrow.  Does it matter?  No, you'll ...</td>\n",
       "      <td>well change scoring playoff pool unfortunately time right certainly post scoring rules tomorrow matter enter anyway good keith keller rangers quakers kkeller mail upenn league champs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                  document  \\\n",
       "0  Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe...   \n",
       "1  \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n...   \n",
       "2  Although I realize that principle is not one of your strongest\\npoints, I would still like to know why do do not ask any question\\nof this sort about the Arab countries.\\n\\n   If you want to conti...   \n",
       "3  Notwithstanding all the legitimate fuss about this proposal, how much\\nof a change is it?  ATT's last product in this area (a) was priced over\\n$1000, as I suspect 'clipper' phones will be; (b) ca...   \n",
       "4  Well, I will have to change the scoring on my playoff pool.  Unfortunately\\nI don't have time right now, but I will certainly post the new scoring\\nrules by tomorrow.  Does it matter?  No, you'll ...   \n",
       "\n",
       "                                                                                                                                                                                                 clean_doc  \n",
       "0  well sure story seem biased disagree statement media ruin israels reputation rediculous media israeli media world lived europe realize incidences described letter occured media whole seem ignore s...  \n",
       "1  yeah expect people read actually accept hard atheism need little leap faith jimmy logic runs steam sorry pity sorry feelings denial faith need well pretend happily ever anyway maybe start newsgrou...  \n",
       "2  although realize principle strongest points would still like know question sort arab countries want continue think tank charade fixation israel must stop might start asking sort questions arab cou...  \n",
       "3  notwithstanding legitimate fuss proposal much change last product area priced suspect clipper phones came customer automatically preregistered government authorities thus aside attempting legitimi...  \n",
       "4                   well change scoring playoff pool unfortunately time right certainly post scoring rules tomorrow matter enter anyway good keith keller rangers quakers kkeller mail upenn league champs  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = news_df.head()\n",
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 9)\\t0.1939619103137807\\n  (0, 5)\\t0.1939619103137807\\n  (0, 4)\\t0.9616431535111439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0, 8)\\t0.3741047724501572\\n  (0, 6)\\t0.9273864454638184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0, 3)\\t0.48096754724600776\\n  (0, 2)\\t0.32064503149733853\\n  (0, 1)\\t0.48096754724600776\\n  (0, 0)\\t0.48096754724600776\\n  (0, 7)\\t0.32064503149733853\\n  (0, 8)\\t0.12934719623338004\\n  (0, 9)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                         0\n",
       "0                                                                                                                   (0, 9)\\t0.1939619103137807\\n  (0, 5)\\t0.1939619103137807\\n  (0, 4)\\t0.9616431535111439\n",
       "1                                                                                                                                                 (0, 8)\\t0.3741047724501572\\n  (0, 6)\\t0.9273864454638184\n",
       "2    (0, 3)\\t0.48096754724600776\\n  (0, 2)\\t0.32064503149733853\\n  (0, 1)\\t0.48096754724600776\\n  (0, 0)\\t0.48096754724600776\\n  (0, 7)\\t0.32064503149733853\\n  (0, 8)\\t0.12934719623338004\\n  (0, 9)\\t...\n",
       "3                                                                                                                                                                                                         \n",
       "4                                                                                                                                                                                                         "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create tf-idf matrix using sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "max_features= 10, # keep top 1000 terms \n",
    "max_df = 0.5, \n",
    "smooth_idf=True)\n",
    "\n",
    "'''max_df = float in range [0.0, 1.0] or int (default=1.0)\n",
    "When building the vocabulary ignore terms that have a document\n",
    "frequency strictly higher than the given threshold (corpus-specific stop words).\n",
    "If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "This parameter is ignored if vocabulary is not None.\n",
    "'''\n",
    "\n",
    "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
    "\n",
    "X.shape # check shape of the document-term matrix\n",
    "\n",
    "x = pd.DataFrame(X)\n",
    "x.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>media</th>\n",
       "      <td>0.961643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realize</th>\n",
       "      <td>0.193962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think</th>\n",
       "      <td>0.193962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arab</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>countries</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixation</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>israel</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sorry</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tfidf\n",
       "media      0.961643\n",
       "realize    0.193962\n",
       "think      0.193962\n",
       "arab       0.000000\n",
       "countries  0.000000\n",
       "fixation   0.000000\n",
       "israel     0.000000\n",
       "sorry      0.000000\n",
       "sort       0.000000\n",
       "start      0.000000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the first vector out (for the first document)\n",
    "first_vector_tfidfvectorizer=X[0]\n",
    " \n",
    "# place tf-idf values in a pandas data frame\n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 189)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# SVD represent documents and terms in vectors \n",
    "svd_model = TruncatedSVD(n_components=5, algorithm='randomized', n_iter=100, random_state=122)\n",
    "\n",
    "svd_model.fit(X)\n",
    "\n",
    "len(svd_model.components_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "media\n",
      " \n",
      "sorry\n",
      " \n",
      "countries\n",
      " \n",
      "israel\n",
      " \n",
      "arab\n",
      " \n",
      "realize\n",
      " \n",
      "start\n",
      " \n",
      "Topic 1: \n",
      "sorry\n",
      " \n",
      "start\n",
      " \n",
      "fixation\n",
      " \n",
      "sort\n",
      " \n",
      "countries\n",
      " \n",
      "israel\n",
      " \n",
      "arab\n",
      " \n",
      "Topic 2: \n",
      "countries\n",
      " \n",
      "arab\n",
      " \n",
      "israel\n",
      " \n",
      "fixation\n",
      " \n",
      "sort\n",
      " \n",
      "realize\n",
      " \n",
      "think\n",
      " \n",
      "Topic 3: \n",
      "fixation\n",
      " \n",
      "countries\n",
      " \n",
      "media\n",
      " \n",
      "sorry\n",
      " \n",
      "start\n",
      " \n",
      "sort\n",
      " \n",
      "israel\n",
      " \n",
      "Topic 4: \n",
      "israel\n",
      " \n",
      "media\n",
      " \n",
      "sorry\n",
      " \n",
      "start\n",
      " \n",
      "think\n",
      " \n",
      "realize\n",
      " \n",
      "sort\n",
      " \n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(svd_model.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_terms:\n",
    "        print(t[0])\n",
    "        print(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'umap' has no attribute 'UMAP'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-f50547fd9984>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_topics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvd_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mumap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUMAP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_dist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_topics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'umap' has no attribute 'UMAP'"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "\n",
    "X_topics = svd_model.fit_transform(X)\n",
    "embedding = umap.UMAP(n_neighbors=150, min_dist=0.5, random_state=12).fit_transform(X_topics)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], \n",
    "c = dataset.target,\n",
    "s = 10, # size\n",
    "edgecolor='none'\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
